#!/usr/bin/env Rscript

library(mxnet)

## Tutorials: neural network in 5 minutes:
require(mlbench)
data(Sonar, package="mlbench")

Sonar[,61] = as.numeric(Sonar[,61]) - 1
train.ind = c(1:50, 100:150)
train.x = data.matrix(Sonar[train.ind, 1:60])
train.y = Sonar[train.ind, 61]
test.x = data.matrix(Sonar[-train.ind, 1:60])
test.y = Sonar[-train.ind, 61]

mx.set.seed(0)
model <- mx.mlp(train.x, train.y, hidden_node=10, out_node=2, out_activation="softmax",
                num.round=20, array.batch.size=15, learning.rate=0.07, momentum=0.9,
                eval.metric=mx.metric.accuracy)

preds = predict(model, test.x)
pred.label = max.col(t(preds))-1
table(pred.label, test.y)

## FFNN test for iris classification

data(iris)
in_train = c(1:30, 51:80, 101:130)
diris = data.matrix(iris)
diris.in = diris[,1:4]
diris.in = diris.in/max(diris.in)
train.x = diris.in[in_train,]
train.y = diris[in_train, 5]
test.x  = diris.in[-in_train,]
test.y  = diris[-in_train, 5]

data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=10)
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu")
## In the output layer, we put as many nodes (neurons) as classes (e.g. iris => 3, mnist => 10)
## so that they act as probabilities. This means the evaluation metric compares the outputs with
## a sparse vector: (0, 0, ..., 1, ... 0).
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=3)
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm")

devices = mx.cpu()

model = mx.model.FeedForward.create(
    softmax, X = train.x, y = train.y, ctx = devices,
    num.round = 15, array.batch.size = 100, learning.rate = 0.02, momentum = 0.5,
    eval.metric = mx.metric.accuracy, initializer = mx.init.uniform(0.07),
    epoch.end.callback = mx.callback.log.train.metric(100))

preds = predict(model, test.x)
pred.label <- max.col(t(preds)) - 1
table(pred.label, test.y)

## FFNN test for regression

data(BostonHousing, package="mlbench")

train.ind = seq(1, 506, 3)
y = c(13, 14)
train.x = data.matrix(BostonHousing[train.ind, -y])
train.y = data.matrix(BostonHousing[train.ind, y])
test.x = data.matrix(BostonHousing[-train.ind, -y])
test.y = data.matrix(BostonHousing[-train.ind, y])

## Define the input data
data <- mx.symbol.Variable("data")
## A fully connected hidden layer
## data: input source
## num_hidden: number of neurons in this hidden layer
fc1 <- mx.symbol.FullyConnected(data, num_hidden=16)
fc2 <- mx.symbol.FullyConnected(data, num_hidden=2)

## Use linear regression for the output layer
lro <- mx.symbol.LinearRegressionOutput(fc2)

mx.set.seed(0)
model <- mx.model.FeedForward.create(lro, X=t(train.x), y=t(train.y),
                                     ctx=mx.cpu(), num.round=50, array.batch.size=20,
                                     learning.rate=2e-6, momentum=0.9,  eval.metric=mx.metric.rmse,
                                     array.layout = "colmajor")
## Notes: if I don't transpose the data and use `array.layout = "rowmajor"`, this error is reported:
##   Error in mx.io.internal.arrayiter(as.array(data), as.array(label), unif.rnds,  : 
##     io.cc:54: Data and label shape in-consistent
## Works fine with colmajor though

preds = predict(model, test.x)

## Multiple-output regression with iris
data <- mx.symbol.Variable("data")
fc1 <- mx.symbol.FullyConnected(data, num_hidden=16)
fc2 <- mx.symbol.FullyConnected(fc1, num_hidden=2)
out <- mx.symbol.LinearRegressionOutput(data = fc2)

model <- mx.model.FeedForward.create(X = t(data.matrix(iris[,-c(1, 2)])),
                                     y = t(data.matrix(iris[, c(1,2)])),
                                     symbol = out,
                                     num.round = 10,
                                     learning.rate = 0.001,
                                     momentum = 0.9,
                                     eval.metric = mx.metric.rmse, 
                                     array.batch.size = 10,
                                     array.layout = "colmajor")

## Autoencoder tests #####################

## In an autoencoder, I need as many outputs as inputs, 

## Fake autoencoder WIP
data(iris)
train.x = t(data.matrix(iris[, 1:4]))
train.y = train.x # same output as input

nn <- mx.symbol.Variable("data")
nn <- mx.symbol.FullyConnected(nn, num_hidden = 2, name = "fc_encoding")
nn <- mx.symbol.FullyConnected(nn, num_hidden = 4, name = "fc_out")
nn <- mx.symbol.LinearRegressionOutput(data = nn, name = "reg_out")
## Note: if I don't add the RegressionOutput to the nn structure I get this error:
##  Error in exec$update.arg.arrays(arg.arrays, match.name, skip.null) : 
##    executor.cc:65: RCheck failed: to->containsElementNamed(names[i].c_str()) cannot find key NA in the array arg.arrays


ae <- mx.model.FeedForward.create(nn, X = train.x, y = train.x,
                                  num.round = 100, learning.rate = 0.001, momentum = 0.9,
                                  eval.metric = mx.metric.rmse, array.layout = "colmajor", optimizer = mxnet:::mx.opt.rmsprop(0.001))

internals = ae$symbol$get.internals()
out.names = internals$outputs
encoded_layer = internals$get.output()


## Not working:
data(iris)
uniris = iris[, 0:4]

## Imperative autoencoder:
hidden = 2
weight = mx.nd.uniform(shape = c(hidden, 4))
bias   = mx.nd.uniform(shape = hidden)
input  = mx.nd.array(as.matrix(uniris))
output = mx.nd.zeros(c(length(uniris[,1]), hidden))

lol = mx.nd.FullyConnected(data = input, weight = weight, bias = bias, num_hidden = hidden, out = output)
net = mx.nd.Activation(data = net, act_type = "tanh", name = "act_hidden")
## decoder:
net = mx.nd.FullyConnected(data = net, num_hidden = 4, name = "output")



## mx.mlp(uniris, uniris, hidden_node = 2, out_node = 4, out_activation = "softmax")

                                        # make encoder:
net = mx.symbol.Variable("data")
net = mx.symbol.FullyConnected(data = net, num_hidden = 4, name = "input")
net = mx.symbol.Activation(data = net, act_type = "tanh", name = "act_input")
net = mx.symbol.FullyConnected(data = net, num_hidden = 2, name = "hidden")
net = mx.symbol.Activation(data = net, act_type = "tanh", name = "act_hidden")
## make decoder:
net = mx.symbol.FullyConnected(data = net, num_hidden = 4, name = "output")
train.x = mx.nd.array(as.matrix(iris[,-5]))
train.y = mx.nd.array(as.array(iris[,5]))
data = mx.io.arrayiter(train.x, train.y, batch.size = 150)
model = mx.model.FeedForward.create(
    net,
    X = data,
    num.round = 2,
    learning.rate = 0.01
)

## make classification NN
net = mx.symbol.Variable("data")
net = mx.symbol.FullyConnected(data = net, num_hidden = 4, name = "input")
net = mx.symbol.Activation(data = net, act_type = "tanh", name = "act_input")
net = mx.symbol.FullyConnected(data = net, num_hidden = 2, name = "hidden")
net = mx.symbol.Activation(data = net, act_type = "tanh", name = "act_hidden")
## make decoder:
net = mx.symbol.FullyConnected(data = net, num_hidden = 1, name = "output")
model = mx.model.FeedForward.create(
    net,
    X = uniris,
    y = iris[,5],
    num_epoch = 10,
    learning_rate = 0.01
)
